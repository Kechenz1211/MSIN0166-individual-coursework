!pip install langchain huggingface_hub

from getpass import getpass
HUGGINGFACE_API_TOKEN=getpass()

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"]=HUGGINGFACE_API_TOKEN
from langchain_community.llms import HuggingFaceHub

from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
# test the model
question="Where is the capital of China?"
template=""" Question: {question} Answer:let's think step by step."""

prompt=PromptTemplate(template=template,input_variables=['question'])
repo_id='google/flan-t5-base'

llm=HuggingFaceHub(
    repo_id=repo_id
)
llm_chain=LLMChain(prompt=prompt,llm=llm,llm_kwargs={"temperature":0,"max_length":512})
print(llm_chain.run(question))

from langchain.document_loaders.csv_loader import CSVLoader
#from langchain.vectorstores import FAISS
from langchain_community.vectorstores import FAISS

from langchain.document_loaders import PyPDFLoader

import pandas as pd

loader = CSVLoader(file_path="merged_data_new_insert.csv")
pages= loader.load()


from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter= RecursiveCharacterTextSplitter(chunk_size=300,chunk_overlap=50,)
docs=text_splitter.split_documents(pages)

# check whether the docs is empty
if not docs:
    print("the splitted file is empty")
else:
    print(f"generate {len(docs)} files")

print(f"Loaded {len(pages)} pages.")
print(f"Split into {len(docs)} documents.")

doc = docs[0]
print(dir(doc))
print(doc)
print(docs)

os.environ["HUGGINGFACE_API_TOKEN"] = HUGGINGFACE_API_TOKEN

dir(docs)

print(type(docs[0]))
print(dir(docs[0]))
texts = [doc.page_content for doc in docs]

from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_community.vectorstores import FAISS

embeddings=HuggingFaceInferenceAPIEmbeddings(
    api_key=HUGGINGFACE_API_TOKEN, model_name='BAAI/bge-base-en-v1.5'
)
# test whether the embeddings work
text = "This is a test document."
query_result = embeddings.embed_query(text)
query_result

!pip install faiss-gpu # this package is required to use the FAISS

## Create a FAISS database from documents and their embeddings.
# This database will be used for similarity searches.
db=FAISS.from_documents(docs,embeddings)

# Define a query to search within the documents.
query='Who is the author of Chani is the real protagonist of Dune: Part Two?'

# Print the page content of the first document for inspection purposes.
print(docs[0].page_content)

# Perform a similarity search in the FAISS database with the specified query
# and retrieving the top 3 similar items.
result_simi = db.similarity_search(query , k = 3)
# Aggregate the page contents of the top 3 similar items into a single string, separated by new lines.
# This aggregated content serves as the source knowledge context for answering the query.
source_knowledge = "\n".join([x.page_content for x in result_simi])

# Construct an augmented prompt that includes both the source knowledge context and the query.
# This prompt is structured to facilitate the generation of an answer based on the provided context.
augmented_prompt = """Using the contexts below, answer the query.

contexts:
{source_knowledge}

query: {query}"""
prompt = PromptTemplate(template=augmented_prompt, input_variables=["source_knowledge" ,"query"])

llm_chain = LLMChain(prompt=prompt, llm=llm  , llm_kwargs = {"temperature":0, "max_length":1024})

print(llm_chain.run( {"source_knowledge":source_knowledge ,"query" : query }))
query2="Linda Bean died at the age of ?"
result_simi2 = db.similarity_search(query2 , k = 5)
source_knowledge2 = "\n".join([x.page_content for x in result_simi2])
augmented_prompt2 = """Using the contexts below, answer the query.
contexts:
{source_knowledge2}

query: {query2}"""
prompt2 = PromptTemplate(template=augmented_prompt2, input_variables=["source_knowledge2" ,"query2"])
llm_chain2 = LLMChain(prompt=prompt2, llm=llm  , llm_kwargs = {"temperature":0.6, "max_length":1024})
print(llm_chain2.run( {"source_knowledge2":source_knowledge2 ,"query2" : query2 }))




